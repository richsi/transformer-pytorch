## transformer-pytorch

Implementation of [Attention Is All You Need](https://arxiv.org/abs/1706.03762).

## Usage

## TODO

- [ ] Encoder
- [ ] Decoder
- [ ] Compare and contrast sinusoidal positionale encoding vs rotary positional encoding (RoPE)

## Citations

```bibtex
@misc{vaswani2017attention,
    title   = {Attention Is All You Need},
    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year    = {2017},
    eprint  = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}
```